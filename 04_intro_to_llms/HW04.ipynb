{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for Science Training - Week 04\n",
    "\n",
    "### Damyn Chipman - Boise State University\n",
    "\n",
    "## Intro to Large Language Models\n",
    "\n",
    "## Homework: Tokenizers\n",
    "\n",
    "### Part 1: **Tokenization** \n",
    "\n",
    "Write a generic Python tokenizer, which takes a set of text lines and tabulates the different words (that is, the tokens will be simply English words), keeping track of the frequency of each word.\n",
    "\n",
    "#### Part 1a.\n",
    "\n",
    "Insert code in this loop to operate on the str variable 'line' so as to fix these problems before 'line' is split into words.\n",
    "\n",
    "A hint to one possible way to do this: use the 'punctuation' character definition in the Python 'string' module, the 'maketrans' and 'translate' methods of Python's str class, to eliminate punctuation, and the regular expression ('re') Python module to eliminate any Unicode---it is useful to know that the regular expression r'[^\\x00-x7f]' means \"any character not in the vanilla ASCII set.\n",
    "\n",
    "#### Part 1b.\n",
    "\n",
    "Add code to sort the contents of wdict by word occurrence frequency.  What are the top 100 most frequent word tokens?  Adding up occurrence frequencies starting from the most frequent words, how many distinct words make up the top 90% of word occurrences in this \"corpus\"?\n",
    "\n",
    "### Part 2: **Embedding**\n",
    "\n",
    "Modify the embedding visualization code above to zoom in on various regions of the projections, and identify at least one interesting cluster of tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## Homework Submission\n",
    "\n",
    "### Part 1a:\n",
    "\n",
    "Let's write an elementary tokenizer that uses words as tokens.\n",
    "\n",
    "We will use Mark Twain's _Life On The Mississippi_ as a test bed. The text is in the accompanying file 'Life_On_The_Mississippi.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 9362)\n",
      "('project', 90)\n",
      "('gutenberg', 97)\n",
      "('ebook', 13)\n",
      "('of', 4541)\n",
      "('life', 94)\n",
      "('on', 962)\n",
      "('mississippi', 165)\n",
      "('this', 794)\n",
      "('is', 1153)\n",
      "('for', 1119)\n",
      "('use', 50)\n",
      "('anyone', 5)\n",
      "('anywhere', 18)\n",
      "('in', 2617)\n",
      "('united', 37)\n",
      "('states', 51)\n",
      "('and', 6032)\n",
      "('most', 125)\n",
      "('other', 271)\n",
      "('parts', 9)\n",
      "('world', 73)\n",
      "('at', 753)\n",
      "('no', 443)\n",
      "('cost', 26)\n",
      "('with', 1095)\n",
      "('almost', 38)\n",
      "('restrictions', 2)\n",
      "('whatsoever', 2)\n",
      "('you', 1043)\n",
      "('may', 92)\n",
      "('copy', 17)\n",
      "('it', 2351)\n",
      "('give', 82)\n",
      "('away', 175)\n",
      "('or', 592)\n",
      "('re', 5)\n",
      "('under', 122)\n",
      "('terms', 27)\n",
      "('license', 27)\n",
      "('included', 3)\n",
      "('online', 4)\n",
      "('www', 9)\n",
      "('org', 9)\n",
      "('if', 382)\n",
      "('are', 387)\n",
      "('not', 734)\n",
      "('located', 9)\n",
      "('will', 302)\n",
      "('have', 570)\n",
      "('to', 3624)\n",
      "('check', 4)\n",
      "('laws', 20)\n",
      "('country', 77)\n",
      "('where', 177)\n",
      "('before', 213)\n",
      "('using', 11)\n",
      "('title', 3)\n",
      "('author', 3)\n",
      "('mark', 19)\n",
      "('twain', 25)\n",
      "('release', 1)\n",
      "('date', 18)\n",
      "('july', 7)\n",
      "('10', 11)\n",
      "('2004', 1)\n",
      "('245', 1)\n",
      "('recently', 4)\n",
      "('updated', 2)\n",
      "('january', 3)\n",
      "('1', 62)\n",
      "('2021', 1)\n",
      "('language', 13)\n",
      "('english', 13)\n",
      "('credits', 1)\n",
      "('produced', 22)\n",
      "('by', 743)\n",
      "('david', 2)\n",
      "('widger', 2)\n",
      "('earliest', 7)\n",
      "('pg', 3)\n",
      "('text', 4)\n",
      "('edition', 4)\n",
      "('graham', 2)\n",
      "('allan', 2)\n",
      "('start', 32)\n",
      "('table', 10)\n",
      "('contents', 6)\n",
      "('chapter', 125)\n",
      "('i', 2281)\n",
      "('well', 203)\n",
      "('worth', 37)\n",
      "('reading', 16)\n",
      "('about', 353)\n",
      "('remarkable', 14)\n",
      "('instead', 25)\n",
      "('widening', 2)\n",
      "('towards', 9)\n",
      "('its', 281)\n",
      "('mouth', 54)\n"
     ]
    }
   ],
   "source": [
    "wdict = {}\n",
    "with open('Life_On_The_Mississippi.txt', 'r') as L:\n",
    "    line = L.readline()\n",
    "    nlines = 1\n",
    "    while line:\n",
    "        words = re.findall(r'\\b[\\w\\']+\\b', line.lower())\n",
    "        for word in words:\n",
    "            if wdict.get(word) is not None:\n",
    "                wdict[word] += 1\n",
    "            else:\n",
    "                wdict[word] = 1\n",
    "        line = L.readline()\n",
    "        nlines += 1\n",
    "\n",
    "nitem = 0 ; maxitems = 100\n",
    "for item in wdict.items():\n",
    "    nitem += 1\n",
    "    print(item)\n",
    "    if nitem == maxitems: break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 9362)\n",
      "('and', 6032)\n",
      "('of', 4541)\n",
      "('a', 4230)\n",
      "('to', 3624)\n",
      "('in', 2617)\n",
      "('it', 2351)\n",
      "('i', 2281)\n",
      "('was', 2097)\n",
      "('that', 1744)\n",
      "('he', 1429)\n",
      "('is', 1153)\n",
      "('for', 1119)\n",
      "('with', 1095)\n",
      "('you', 1043)\n",
      "('but', 986)\n",
      "('his', 965)\n",
      "('on', 962)\n",
      "('had', 960)\n",
      "('as', 886)\n",
      "('this', 794)\n",
      "('they', 767)\n",
      "('at', 753)\n",
      "('by', 743)\n",
      "('all', 735)\n",
      "('not', 734)\n",
      "('one', 715)\n",
      "('there', 642)\n",
      "('were', 627)\n",
      "('be', 620)\n",
      "('or', 592)\n",
      "('my', 586)\n",
      "('from', 579)\n",
      "('have', 570)\n",
      "('so', 557)\n",
      "('out', 553)\n",
      "('up', 547)\n",
      "('me', 536)\n",
      "('we', 531)\n",
      "('him', 529)\n",
      "('when', 506)\n",
      "('which', 491)\n",
      "('river', 486)\n",
      "('would', 478)\n",
      "('an', 455)\n",
      "('no', 443)\n",
      "('them', 431)\n",
      "('then', 419)\n",
      "('said', 404)\n",
      "('are', 387)\n",
      "('if', 382)\n",
      "('their', 377)\n",
      "('now', 377)\n",
      "('time', 355)\n",
      "('about', 353)\n",
      "('down', 342)\n",
      "('been', 336)\n",
      "('could', 312)\n",
      "('has', 306)\n",
      "('will', 302)\n",
      "('two', 301)\n",
      "('into', 300)\n",
      "('what', 299)\n",
      "('her', 282)\n",
      "('its', 281)\n",
      "('some', 274)\n",
      "('do', 272)\n",
      "('other', 271)\n",
      "('new', 270)\n",
      "('man', 265)\n",
      "('water', 245)\n",
      "('she', 241)\n",
      "('any', 239)\n",
      "('more', 234)\n",
      "('got', 233)\n",
      "('these', 233)\n",
      "('who', 231)\n",
      "('day', 220)\n",
      "('way', 217)\n",
      "('did', 215)\n",
      "('before', 213)\n",
      "('boat', 213)\n",
      "('here', 210)\n",
      "('over', 206)\n",
      "('hundred', 204)\n",
      "('well', 203)\n",
      "('old', 200)\n",
      "('upon', 200)\n",
      "('after', 198)\n",
      "('good', 194)\n",
      "('through', 194)\n",
      "('than', 192)\n",
      "('get', 191)\n",
      "('every', 187)\n",
      "('can', 187)\n",
      "('went', 184)\n",
      "('never', 184)\n",
      "('see', 183)\n",
      "('years', 182)\n",
      "('off', 181)\n"
     ]
    }
   ],
   "source": [
    "sorted_wdict = dict(sorted(wdict.items(), key=lambda item: item[1], reverse=True))\n",
    "nitem = 0 ; maxitems = 100\n",
    "for item in sorted_wdict.items():\n",
    "    nitem += 1\n",
    "    print(item)\n",
    "    if nitem == maxitems: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150899"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words = sum([word_freq for word_freq in sorted_wdict.values()])\n",
    "n_top_90_words = n_words * 0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2:\n",
    "\n",
    "Here's the original projection of the embeddings:\n",
    "\n",
    "<img src=viz-bert-voc-tsne10k-viz4k-noadj.pdf />\n",
    "\n",
    "That weird string clusters turns out to be the years listed. Note that they are not just numbers, but in fact dates:\n",
    "\n",
    "<img src=embedding-01.png />\n",
    "\n",
    "I found a cluster of cardinal directions that are not, in fact, central:\n",
    "\n",
    "<img src=embedding-02.png />\n",
    "\n",
    "And it looks like all of the names are somewhat clustered. Names typically associated with males versus females are also clustered within this cluster of names. The names also somewhat lead into the names of places as that is how we sometimes name places.\n",
    "\n",
    "<img src=embedding-03.png />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
